{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - you can add any other permitted libraries\n",
    "import numpy as np\n",
    "\n",
    "# You may add any other functions to make your code more modular. However,\n",
    "# do not change the function signatures (name and arguments) of the given functions,\n",
    "# as these functions will be called by the autograder.\n",
    "\n",
    "class LogisticRegressor:\n",
    "    # Assume Binary Classification\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    \n",
    "    def _normalize(self, X):\n",
    "        \"\"\"Normalize features using mean and standard deviation.\"\"\"\n",
    "        mean = np.mean(X, axis=0)\n",
    "        std = np.std(X, axis=0)\n",
    "        return (X - mean) / std\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute the sigmoid function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_gradient(self, X, y, h):\n",
    "        \"\"\"Compute the gradient of the log-likelihood function.\"\"\"\n",
    "        return X.T @ (h - y)\n",
    "    \n",
    "    def _compute_hessian(self, X, h):\n",
    "        \"\"\"Compute the Hessian matrix of the log-likelihood function.\"\"\"\n",
    "        S = np.diag(h * (1 - h))  # Diagonal matrix of weights\n",
    "        return X.T @ S @ X\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.01, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model to the data using Newton's Method.\n",
    "        Remember to normalize the input data X before fitting the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "            \n",
    "        y : numpy array of shape (n_samples,)\n",
    "            The target labels - 0 or 1.\n",
    "        \n",
    "        learning_rate : float\n",
    "            The learning rate to use in the update rule.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List of Parameters: numpy array of shape (n_iter, n_features+1,)\n",
    "            The list of parameters obtained after each iteration of Newton's Method.\n",
    "        \"\"\"\n",
    "        X = self._normalize(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        X = np.c_[np.ones(n_samples), X]  # Add bias term\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        theta_list = []\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            z = X @ self.theta\n",
    "            h = self._sigmoid(z)\n",
    "            gradient = self._compute_gradient(X, y, h)\n",
    "            hessian = self._compute_hessian(X, h)\n",
    "            \n",
    "            # Newton's update step: theta = theta - H^(-1) * gradient\n",
    "            theta_update = np.linalg.inv(hessian) @ gradient\n",
    "            self.theta -= theta_update\n",
    "            theta_list.append(self.theta.copy())\n",
    "            \n",
    "            # Convergence check\n",
    "            if np.linalg.norm(theta_update) < tol:\n",
    "                break\n",
    "        \n",
    "        return np.array(theta_list)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : numpy array of shape (n_samples,)\n",
    "            The predicted target label.\n",
    "        \"\"\"\n",
    "        X = self._normalize(X)\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        probabilities = self._sigmoid(X @ self.theta)\n",
    "        return (probabilities >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final theta values: [ 0.40125316  2.5885477  -2.72558849]\n"
     ]
    }
   ],
   "source": [
    "# Load data and compute theta values\n",
    "X = np.loadtxt('../data/Q3/logisticX.csv', delimiter=',')\n",
    "y = np.loadtxt('../data/Q3/logisticY.csv', delimiter=',')\n",
    "\n",
    "model = LogisticRegressor()\n",
    "theta_values = model.fit(X, y)\n",
    "print(\"Final theta values:\", model.theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
